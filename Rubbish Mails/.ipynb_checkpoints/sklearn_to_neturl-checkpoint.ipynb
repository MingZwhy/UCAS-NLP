{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93f639fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a4ffe931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ID label                                            context\n",
      "1  0   ham  I don't have anybody's number, I still haven't...\n",
      "2  1  spam  Congrats! 2 mobile 3G Videophones R yours. cal...\n",
      "3  2   ham  She is our sister.. She belongs 2 our family.....\n",
      "4  3   ham              Ya very nice. . .be ready on thursday\n",
      "5  4   ham                                               Okie\n"
     ]
    }
   ],
   "source": [
    "data_init = pd.read_csv(r'F:\\BaiduNetdiskDownload\\rb_mails\\train.csv', sep=',', names=['ID', 'label','context'])\n",
    "data_init.drop(index=0, inplace=True)\n",
    "print(data_init.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca957a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_init['label'] = data_init.label.map({'ham': 0, 'spam': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd3a0e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data_init['context'], data_init['label'], random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ffd21664",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector = CountVectorizer(stop_words='english')\n",
    "train_data = count_vector.fit_transform(x_train)\n",
    "test_data = count_vector.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d730a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(solver='lbfgs', activation='logistic')\n",
    "mlp.fit(train_data, y_train)\n",
    "predictions_nn = mlp.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9f78b32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neural network Accuracy score: 0.9838565022421525\n",
      "neural network Precision score: 0.9765625\n",
      "neural network Recall score: 0.8928571428571429\n",
      "neural network F1 score: 0.9328358208955224\n"
     ]
    }
   ],
   "source": [
    "print('neural network Accuracy score:', format(accuracy_score(y_test, predictions_nn)))\n",
    "print('neural network Precision score:', format(precision_score(y_test, predictions_nn)))\n",
    "print('neural network Recall score:', format(recall_score(y_test, predictions_nn)))\n",
    "print('neural network F1 score:', format(f1_score(y_test, predictions_nn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a37fe0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ID                                              label  context\n",
      "1  4458               Ok.ok ok..then..whats ur todays plan      NaN\n",
      "2  4459  \\Hi darlin did youPhone me? Im atHome if youwa...      NaN\n",
      "3  4460                   K. Did you call me just now ah?       NaN\n",
      "4  4461  Urgent UR awarded a complimentary trip to Euro...      NaN\n",
      "5  4462                               ?called dad oredi...      NaN\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1114"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_no_label = pd.read_csv(r'F:\\BaiduNetdiskDownload\\rb_mails\\test_noLabel.csv', sep=',', names=['ID', 'context','label'])\n",
    "data_no_label.drop(index=0, inplace=True)\n",
    "print(data_no_label.head())\n",
    "len(data_no_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3f2ebcd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m test_no_label, label_no_label \u001b[38;5;241m=\u001b[39m data_no_label[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m], data_no_label[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m test_data_no_label \u001b[38;5;241m=\u001b[39m \u001b[43mcount_vector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_no_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m predictions_nn \u001b[38;5;241m=\u001b[39m mlp\u001b[38;5;241m.\u001b[39mpredict(test_data_no_label)\n",
      "File \u001b[1;32mF:\\Anaconda3\\envs\\PyTorch\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1433\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[0;32m   1432\u001b[0m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[1;32m-> 1433\u001b[0m _, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1435\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mF:\\Anaconda3\\envs\\PyTorch\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1275\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1273\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1274\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1275\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1276\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mF:\\Anaconda3\\envs\\PyTorch\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32mF:\\Anaconda3\\envs\\PyTorch\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:69\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 69\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "test_no_label, label_no_label = data_no_label['context'], data_no_label['label']\n",
    "test_data_no_label = count_vector.transform(test_no_label)\n",
    "predictions_nn = mlp.predict(test_data_no_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0ea6081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4458"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7f97428",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item,pred in enumerate(predictions_nn):\n",
    "    data_no_label.loc[item,'label'] = 'ham' if pred==0 else 'spam'\n",
    "data_no_label.to_csv(r'F:\\BaiduNetdiskDownload\\rb_mails\\mlp\\out.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99603aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PyTorch] *",
   "language": "python",
   "name": "conda-env-PyTorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
